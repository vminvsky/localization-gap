{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78748d6a4c8f4d428efcb7f49123706d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.71 GiB. GPU 0 has a total capacity of 39.49 GiB of which 80.19 MiB is free. Process 2835212 has 676.00 MiB memory in use. Process 489498 has 524.00 MiB memory in use. Process 491959 has 536.00 MiB memory in use. Process 577190 has 536.00 MiB memory in use. Process 892239 has 536.00 MiB memory in use. Process 1946005 has 1.09 GiB memory in use. Process 2503297 has 1.71 GiB memory in use. Process 2513956 has 32.40 GiB memory in use. Process 2633610 has 416.00 MiB memory in use. Process 3233263 has 614.00 MiB memory in use. Including non-PyTorch memory, this process has 414.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mt\u001b[39;00m \n\u001b[1;32m     11\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/scratch/gpfs/vv7118/models/hub/models--google--gemma-2-9b-it/snapshots/11c9b309abf73637e4b6f9a3fa1e92e615547819/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m nnmodel \u001b[38;5;241m=\u001b[39m \u001b[43mLanguageModel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/scratch/gpfs/vv7118/models/hub/models--google--gemma-2-9b-it/snapshots/11c9b309abf73637e4b6f9a3fa1e92e615547819/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mdispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/mech-int/lib/python3.11/site-packages/nnsight/models/LanguageModel.py:160\u001b[0m, in \u001b[0;36mLanguageModel.__init__\u001b[0;34m(self, model_key, tokenizer, automodel, *args, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model_key, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(model_key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator\u001b[39m\u001b[38;5;124m\"\u001b[39m, WrapperModule())\n\u001b[0;32m--> 160\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mech-int/lib/python3.11/site-packages/nnsight/models/NNsightModel.py:119\u001b[0m, in \u001b[0;36mNNsight.__init__\u001b[0;34m(self, model_key, dispatch, meta_buffers, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_envoy \u001b[38;5;241m=\u001b[39m Envoy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dispatch \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatched:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# Dispatch ._model on initialization vs lazy dispatching.\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitialized `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mech-int/lib/python3.11/site-packages/nnsight/models/NNsightModel.py:494\u001b[0m, in \u001b[0;36mNNsight.dispatch_model\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Dispatch ._model to have real parameters  using ._load(...).\"\"\"\u001b[39;00m\n\u001b[1;32m    492\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDispatching `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_kwargs\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_envoy\u001b[38;5;241m.\u001b[39m_update(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model)\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mech-int/lib/python3.11/site-packages/nnsight/models/LanguageModel.py:212\u001b[0m, in \u001b[0;36mLanguageModel._load\u001b[0;34m(self, repo_id, tokenizer_kwargs, patch_llama_scan, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    205\u001b[0m     patch_llama_scan\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, LlamaConfig)\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config\u001b[38;5;241m.\u001b[39mrope_scaling, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrope_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mrope_scaling\n\u001b[1;32m    209\u001b[0m ):\n\u001b[1;32m    210\u001b[0m     config\u001b[38;5;241m.\u001b[39mrope_scaling[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrope_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 212\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28msetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator\u001b[39m\u001b[38;5;124m\"\u001b[39m, WrapperModule())\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/mech-int/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/mech-int/lib/python3.11/site-packages/transformers/modeling_utils.py:4014\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4004\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4005\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   4007\u001b[0m     (\n\u001b[1;32m   4008\u001b[0m         model,\n\u001b[1;32m   4009\u001b[0m         missing_keys,\n\u001b[1;32m   4010\u001b[0m         unexpected_keys,\n\u001b[1;32m   4011\u001b[0m         mismatched_keys,\n\u001b[1;32m   4012\u001b[0m         offload_index,\n\u001b[1;32m   4013\u001b[0m         error_msgs,\n\u001b[0;32m-> 4014\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   4018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4021\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4022\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4025\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4026\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4030\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4031\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4033\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   4034\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/miniconda3/envs/mech-int/lib/python3.11/site-packages/transformers/modeling_utils.py:4502\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path)\u001b[0m\n\u001b[1;32m   4498\u001b[0m                 set_module_tensor_to_device(\n\u001b[1;32m   4499\u001b[0m                     model_to_load, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m*\u001b[39mparam\u001b[38;5;241m.\u001b[39msize(), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   4500\u001b[0m                 )\n\u001b[1;32m   4501\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4502\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4503\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4504\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4505\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4506\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4507\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4508\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4509\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4510\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4511\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4512\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4513\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4514\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4515\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4516\u001b[0m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4517\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4518\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   4519\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4520\u001b[0m     \u001b[38;5;66;03m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mech-int/lib/python3.11/site-packages/transformers/modeling_utils.py:973\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys, pretrained_model_name_or_path)\u001b[0m\n\u001b[1;32m    970\u001b[0m         param_device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    972\u001b[0m     \u001b[38;5;66;03m# For backward compatibility with older versions of `accelerate` and for non-quantized params\u001b[39;00m\n\u001b[0;32m--> 973\u001b[0m     \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mset_module_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    975\u001b[0m     hf_quantizer\u001b[38;5;241m.\u001b[39mcreate_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)\n",
      "File \u001b[0;32m~/miniconda3/envs/mech-int/lib/python3.11/site-packages/accelerate/utils/modeling.py:330\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    328\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 330\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    332\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.71 GiB. GPU 0 has a total capacity of 39.49 GiB of which 80.19 MiB is free. Process 2835212 has 676.00 MiB memory in use. Process 489498 has 524.00 MiB memory in use. Process 491959 has 536.00 MiB memory in use. Process 577190 has 536.00 MiB memory in use. Process 892239 has 536.00 MiB memory in use. Process 1946005 has 1.09 GiB memory in use. Process 2503297 has 1.71 GiB memory in use. Process 2513956 has 32.40 GiB memory in use. Process 2633610 has 416.00 MiB memory in use. Process 3233263 has 614.00 MiB memory in use. Including non-PyTorch memory, this process has 414.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('llm-localization')\n",
    "from tools.contrastiveact import contrastive_act_gen_opt\n",
    "from tqdm import trange\n",
    "from nnsight import LanguageModel\n",
    "from transformers import AutoTokenizer\n",
    "import torch as t \n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/scratch/gpfs/vv7118/models/hub/models--google--gemma-2-9b-it/snapshots/11c9b309abf73637e4b6f9a3fa1e92e615547819/\")\n",
    "nnmodel = LanguageModel('/scratch/gpfs/vv7118/models/hub/models--google--gemma-2-9b-it/snapshots/11c9b309abf73637e4b6f9a3fa1e92e615547819/', \n",
    "                        device_map='cuda:0', \n",
    "                        dispatch=True, \n",
    "                        torch_dtype=t.bfloat16)\n",
    "\n",
    "alpha = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos><start_of_turn>user\\nВопрос:Чтобы ответить на следующий вопрос с выбором ответа, вы должны выбрать только один вариант из 1,2,3,4. Инструкция: Вы должны выбрать один вариант из 1,2,3,4. Не выводите ничего другого.\\nЯ живу в России. Какое из следующих имен наиболее вероятно моё имя?\\nВарианты:\\n1. Себастьян\\n2<end_of_turn>\\n<start_of_turn>model\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template([{\"role\":\"user\", \"content\":\"Вопрос:Чтобы ответить на следующий вопрос с выбором ответа, вы должны выбрать только один вариант из 1,2,3,4. Инструкция: Вы должны выбрать один вариант из 1,2,3,4. Не выводите ничего другого.\\nЯ живу в России. Какое из следующих имен наиболее вероятно моё имя?\\nВарианты:\\n1. Себастьян\\n2\"}], tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv('data/all_models_eval_subset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_id</th>\n",
       "      <th>question_id</th>\n",
       "      <th>country</th>\n",
       "      <th>question</th>\n",
       "      <th>lang</th>\n",
       "      <th>options</th>\n",
       "      <th>hint</th>\n",
       "      <th>ans_local_idx</th>\n",
       "      <th>ans_west_idx</th>\n",
       "      <th>ans_local</th>\n",
       "      <th>ans_west</th>\n",
       "      <th>subtask</th>\n",
       "      <th>prompt</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>ans_type</th>\n",
       "      <th>model</th>\n",
       "      <th>translated</th>\n",
       "      <th>key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6522</th>\n",
       "      <td>synth_names</td>\n",
       "      <td>synth_names_0_ru</td>\n",
       "      <td>Russia</td>\n",
       "      <td>Я живу в России. Какое из следующих имен наибо...</td>\n",
       "      <td>Russian</td>\n",
       "      <td>['Александр', 'Лиам']</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Александр</td>\n",
       "      <td>Лиам</td>\n",
       "      <td>names</td>\n",
       "      <td>Вопрос:Чтобы ответить на следующий вопрос с вы...</td>\n",
       "      <td>&lt;BOS_TOKEN&gt;&lt;|START_OF_TURN_TOKEN|&gt;&lt;|USER_TOKEN...</td>\n",
       "      <td>1</td>\n",
       "      <td>local</td>\n",
       "      <td>aya_8b_it</td>\n",
       "      <td>True</td>\n",
       "      <td>synth_names_0_ruRussia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6523</th>\n",
       "      <td>synth_names</td>\n",
       "      <td>synth_names_0_ru</td>\n",
       "      <td>Russia</td>\n",
       "      <td>Какое из следующих имен наиболее вероятно моё ...</td>\n",
       "      <td>Russian</td>\n",
       "      <td>['Александр', 'Лиам']</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Александр</td>\n",
       "      <td>Лиам</td>\n",
       "      <td>names</td>\n",
       "      <td>Вопрос:Чтобы ответить на следующий вопрос с вы...</td>\n",
       "      <td>&lt;BOS_TOKEN&gt;&lt;|START_OF_TURN_TOKEN|&gt;&lt;|USER_TOKEN...</td>\n",
       "      <td>1</td>\n",
       "      <td>local</td>\n",
       "      <td>aya_8b_it</td>\n",
       "      <td>True</td>\n",
       "      <td>synth_names_0_ruRussia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6526</th>\n",
       "      <td>synth_names</td>\n",
       "      <td>synth_names_1_ru</td>\n",
       "      <td>Russia</td>\n",
       "      <td>Я живу в России. Какое из следующих имен наибо...</td>\n",
       "      <td>Russian</td>\n",
       "      <td>['Лиам', 'Михаил']</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Михаил</td>\n",
       "      <td>Лиам</td>\n",
       "      <td>names</td>\n",
       "      <td>Вопрос:Чтобы ответить на следующий вопрос с вы...</td>\n",
       "      <td>&lt;BOS_TOKEN&gt;&lt;|START_OF_TURN_TOKEN|&gt;&lt;|USER_TOKEN...</td>\n",
       "      <td>2. Михаил</td>\n",
       "      <td>local</td>\n",
       "      <td>aya_8b_it</td>\n",
       "      <td>True</td>\n",
       "      <td>synth_names_1_ruRussia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6527</th>\n",
       "      <td>synth_names</td>\n",
       "      <td>synth_names_1_ru</td>\n",
       "      <td>Russia</td>\n",
       "      <td>Какое из следующих имен наиболее вероятно моё ...</td>\n",
       "      <td>Russian</td>\n",
       "      <td>['Лиам', 'Михаил']</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Михаил</td>\n",
       "      <td>Лиам</td>\n",
       "      <td>names</td>\n",
       "      <td>Вопрос:Чтобы ответить на следующий вопрос с вы...</td>\n",
       "      <td>&lt;BOS_TOKEN&gt;&lt;|START_OF_TURN_TOKEN|&gt;&lt;|USER_TOKEN...</td>\n",
       "      <td>2. Михаил</td>\n",
       "      <td>local</td>\n",
       "      <td>aya_8b_it</td>\n",
       "      <td>True</td>\n",
       "      <td>synth_names_1_ruRussia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6530</th>\n",
       "      <td>synth_names</td>\n",
       "      <td>synth_names_2_ru</td>\n",
       "      <td>Russia</td>\n",
       "      <td>Я живу в России. Какое из следующих имен наибо...</td>\n",
       "      <td>Russian</td>\n",
       "      <td>['Лиам', 'Максим']</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Максим</td>\n",
       "      <td>Лиам</td>\n",
       "      <td>names</td>\n",
       "      <td>Вопрос:Чтобы ответить на следующий вопрос с вы...</td>\n",
       "      <td>&lt;BOS_TOKEN&gt;&lt;|START_OF_TURN_TOKEN|&gt;&lt;|USER_TOKEN...</td>\n",
       "      <td>2. Максим</td>\n",
       "      <td>local</td>\n",
       "      <td>aya_8b_it</td>\n",
       "      <td>True</td>\n",
       "      <td>synth_names_2_ruRussia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94566</th>\n",
       "      <td>culturebench</td>\n",
       "      <td>culturebench_1224_1</td>\n",
       "      <td>Russia</td>\n",
       "      <td>Почему в России вы даете людям нечетное количе...</td>\n",
       "      <td>Russian</td>\n",
       "      <td>['Потому что четные числа ассоциируются с похо...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Потому что четные числа ассоциируются с похоро...</td>\n",
       "      <td>Поздравить с днем рождения</td>\n",
       "      <td>culturalbench</td>\n",
       "      <td>Вопрос:Чтобы ответить на следующий вопрос с вы...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.</td>\n",
       "      <td>local</td>\n",
       "      <td>llama31_70b_it</td>\n",
       "      <td>True</td>\n",
       "      <td>culturebench_1224_1Russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94567</th>\n",
       "      <td>culturebench</td>\n",
       "      <td>culturebench_1224_2</td>\n",
       "      <td>Russia</td>\n",
       "      <td>Почему в России вы даете людям нечетное количе...</td>\n",
       "      <td>Russian</td>\n",
       "      <td>['Потому что четные числа ассоциируются с похо...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Потому что четные числа ассоциируются с похоро...</td>\n",
       "      <td>Пожелать удачи на новой работе</td>\n",
       "      <td>culturalbench</td>\n",
       "      <td>Вопрос:Чтобы ответить на следующий вопрос с вы...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.</td>\n",
       "      <td>local</td>\n",
       "      <td>llama31_70b_it</td>\n",
       "      <td>True</td>\n",
       "      <td>culturebench_1224_2Russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94568</th>\n",
       "      <td>culturebench</td>\n",
       "      <td>culturebench_1224_0</td>\n",
       "      <td>Russia</td>\n",
       "      <td>Почему вы даете людям нечетное количество цветов?</td>\n",
       "      <td>Russian</td>\n",
       "      <td>['Потому что четные числа ассоциируются с похо...</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Потому что четные числа ассоциируются с похоро...</td>\n",
       "      <td>Выражать симпатию на похоронах</td>\n",
       "      <td>culturalbench</td>\n",
       "      <td>Вопрос:Чтобы ответить на следующий вопрос с вы...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.</td>\n",
       "      <td>local</td>\n",
       "      <td>llama31_70b_it</td>\n",
       "      <td>True</td>\n",
       "      <td>culturebench_1224_0Russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94569</th>\n",
       "      <td>culturebench</td>\n",
       "      <td>culturebench_1224_1</td>\n",
       "      <td>Russia</td>\n",
       "      <td>Почему вы даете людям нечетное количество цветов?</td>\n",
       "      <td>Russian</td>\n",
       "      <td>['Потому что четные числа ассоциируются с похо...</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Потому что четные числа ассоциируются с похоро...</td>\n",
       "      <td>Поздравить с днем рождения</td>\n",
       "      <td>culturalbench</td>\n",
       "      <td>Вопрос:Чтобы ответить на следующий вопрос с вы...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.</td>\n",
       "      <td>west</td>\n",
       "      <td>llama31_70b_it</td>\n",
       "      <td>True</td>\n",
       "      <td>culturebench_1224_1Russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94570</th>\n",
       "      <td>culturebench</td>\n",
       "      <td>culturebench_1224_2</td>\n",
       "      <td>Russia</td>\n",
       "      <td>Почему вы даете людям нечетное количество цветов?</td>\n",
       "      <td>Russian</td>\n",
       "      <td>['Потому что четные числа ассоциируются с похо...</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Потому что четные числа ассоциируются с похоро...</td>\n",
       "      <td>Пожелать удачи на новой работе</td>\n",
       "      <td>culturalbench</td>\n",
       "      <td>Вопрос:Чтобы ответить на следующий вопрос с вы...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.</td>\n",
       "      <td>west</td>\n",
       "      <td>llama31_70b_it</td>\n",
       "      <td>True</td>\n",
       "      <td>culturebench_1224_2Russia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8880 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          source_id          question_id country  \\\n",
       "6522    synth_names     synth_names_0_ru  Russia   \n",
       "6523    synth_names     synth_names_0_ru  Russia   \n",
       "6526    synth_names     synth_names_1_ru  Russia   \n",
       "6527    synth_names     synth_names_1_ru  Russia   \n",
       "6530    synth_names     synth_names_2_ru  Russia   \n",
       "...             ...                  ...     ...   \n",
       "94566  culturebench  culturebench_1224_1  Russia   \n",
       "94567  culturebench  culturebench_1224_2  Russia   \n",
       "94568  culturebench  culturebench_1224_0  Russia   \n",
       "94569  culturebench  culturebench_1224_1  Russia   \n",
       "94570  culturebench  culturebench_1224_2  Russia   \n",
       "\n",
       "                                                question     lang  \\\n",
       "6522   Я живу в России. Какое из следующих имен наибо...  Russian   \n",
       "6523   Какое из следующих имен наиболее вероятно моё ...  Russian   \n",
       "6526   Я живу в России. Какое из следующих имен наибо...  Russian   \n",
       "6527   Какое из следующих имен наиболее вероятно моё ...  Russian   \n",
       "6530   Я живу в России. Какое из следующих имен наибо...  Russian   \n",
       "...                                                  ...      ...   \n",
       "94566  Почему в России вы даете людям нечетное количе...  Russian   \n",
       "94567  Почему в России вы даете людям нечетное количе...  Russian   \n",
       "94568  Почему вы даете людям нечетное количество цветов?  Russian   \n",
       "94569  Почему вы даете людям нечетное количество цветов?  Russian   \n",
       "94570  Почему вы даете людям нечетное количество цветов?  Russian   \n",
       "\n",
       "                                                 options   hint  \\\n",
       "6522                               ['Александр', 'Лиам']   True   \n",
       "6523                               ['Александр', 'Лиам']  False   \n",
       "6526                                  ['Лиам', 'Михаил']   True   \n",
       "6527                                  ['Лиам', 'Михаил']  False   \n",
       "6530                                  ['Лиам', 'Максим']   True   \n",
       "...                                                  ...    ...   \n",
       "94566  ['Потому что четные числа ассоциируются с похо...   True   \n",
       "94567  ['Потому что четные числа ассоциируются с похо...   True   \n",
       "94568  ['Потому что четные числа ассоциируются с похо...  False   \n",
       "94569  ['Потому что четные числа ассоциируются с похо...  False   \n",
       "94570  ['Потому что четные числа ассоциируются с похо...  False   \n",
       "\n",
       "       ans_local_idx  ans_west_idx  \\\n",
       "6522             1.0           2.0   \n",
       "6523             1.0           2.0   \n",
       "6526             2.0           1.0   \n",
       "6527             2.0           1.0   \n",
       "6530             2.0           1.0   \n",
       "...              ...           ...   \n",
       "94566            1.0           2.0   \n",
       "94567            1.0           2.0   \n",
       "94568            1.0           2.0   \n",
       "94569            1.0           2.0   \n",
       "94570            1.0           2.0   \n",
       "\n",
       "                                               ans_local  \\\n",
       "6522                                           Александр   \n",
       "6523                                           Александр   \n",
       "6526                                              Михаил   \n",
       "6527                                              Михаил   \n",
       "6530                                              Максим   \n",
       "...                                                  ...   \n",
       "94566  Потому что четные числа ассоциируются с похоро...   \n",
       "94567  Потому что четные числа ассоциируются с похоро...   \n",
       "94568  Потому что четные числа ассоциируются с похоро...   \n",
       "94569  Потому что четные числа ассоциируются с похоро...   \n",
       "94570  Потому что четные числа ассоциируются с похоро...   \n",
       "\n",
       "                             ans_west        subtask  \\\n",
       "6522                             Лиам          names   \n",
       "6523                             Лиам          names   \n",
       "6526                             Лиам          names   \n",
       "6527                             Лиам          names   \n",
       "6530                             Лиам          names   \n",
       "...                               ...            ...   \n",
       "94566      Поздравить с днем рождения  culturalbench   \n",
       "94567  Пожелать удачи на новой работе  culturalbench   \n",
       "94568  Выражать симпатию на похоронах  culturalbench   \n",
       "94569      Поздравить с днем рождения  culturalbench   \n",
       "94570  Пожелать удачи на новой работе  culturalbench   \n",
       "\n",
       "                                                  prompt  \\\n",
       "6522   Вопрос:Чтобы ответить на следующий вопрос с вы...   \n",
       "6523   Вопрос:Чтобы ответить на следующий вопрос с вы...   \n",
       "6526   Вопрос:Чтобы ответить на следующий вопрос с вы...   \n",
       "6527   Вопрос:Чтобы ответить на следующий вопрос с вы...   \n",
       "6530   Вопрос:Чтобы ответить на следующий вопрос с вы...   \n",
       "...                                                  ...   \n",
       "94566  Вопрос:Чтобы ответить на следующий вопрос с вы...   \n",
       "94567  Вопрос:Чтобы ответить на следующий вопрос с вы...   \n",
       "94568  Вопрос:Чтобы ответить на следующий вопрос с вы...   \n",
       "94569  Вопрос:Чтобы ответить на следующий вопрос с вы...   \n",
       "94570  Вопрос:Чтобы ответить на следующий вопрос с вы...   \n",
       "\n",
       "                                                   input     output ans_type  \\\n",
       "6522   <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN...          1    local   \n",
       "6523   <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN...          1    local   \n",
       "6526   <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN...  2. Михаил    local   \n",
       "6527   <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN...  2. Михаил    local   \n",
       "6530   <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN...  2. Максим    local   \n",
       "...                                                  ...        ...      ...   \n",
       "94566                                                NaN         1.    local   \n",
       "94567                                                NaN         1.    local   \n",
       "94568                                                NaN         1.    local   \n",
       "94569                                                NaN         2.     west   \n",
       "94570                                                NaN         2.     west   \n",
       "\n",
       "                model  translated                        key  \n",
       "6522        aya_8b_it        True     synth_names_0_ruRussia  \n",
       "6523        aya_8b_it        True     synth_names_0_ruRussia  \n",
       "6526        aya_8b_it        True     synth_names_1_ruRussia  \n",
       "6527        aya_8b_it        True     synth_names_1_ruRussia  \n",
       "6530        aya_8b_it        True     synth_names_2_ruRussia  \n",
       "...               ...         ...                        ...  \n",
       "94566  llama31_70b_it        True  culturebench_1224_1Russia  \n",
       "94567  llama31_70b_it        True  culturebench_1224_2Russia  \n",
       "94568  llama31_70b_it        True  culturebench_1224_0Russia  \n",
       "94569  llama31_70b_it        True  culturebench_1224_1Russia  \n",
       "94570  llama31_70b_it        True  culturebench_1224_2Russia  \n",
       "\n",
       "[8880 rows x 19 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['lang'] == 'Russian']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def messages_to_str(messages, tokenizer, instruction_model=True):\n",
    "    if type(messages) == str:\n",
    "        messages = [{\"role\":\"user\", \"content\":messages}]\n",
    "    if instruction_model:\n",
    "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =df[df['country'] == 'Russia'].iloc[3]['input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1965720/193889668.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  steering_vec = t.load('llm-localization/gemma2_9b_it/universal/en_universal_ru_out.pt')\n"
     ]
    }
   ],
   "source": [
    "steering_vec = t.load('llm-localization/gemma2_9b_it/universal/en_universal_ru_out.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma2ForCausalLM(\n",
       "  (model): Gemma2Model(\n",
       "    (embed_tokens): Embedding(256000, 3584, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-41): 42 x Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2Attention(\n",
       "          (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "          (rotary_emb): Gemma2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=256000, bias=False)\n",
       "  (generator): WrapperModule()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([42, 3584])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steering_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1965720/4163855927.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  steering_vec = t.load('llm-localization/gemma2_9b_it/universal/en_universal_ru_out.pt')\n",
      "You're using a GemmaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [1, 13, 256000] at entry 0 and [1, 18, 256000] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m steering_vec \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllm-localization/gemma2_9b_it/universal/en_universal_ru_out.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mcontrastive_act_gen_opt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnnmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msteering_vec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m21\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m22\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m23\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_sampling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/gpfs/vv7118/projects/localization-gap/llm-localization/tools/contrastiveact.py:174\u001b[0m, in \u001b[0;36mcontrastive_act_gen_opt\u001b[0;34m(nnmodel, tokenizer, intervene_vec, intervene_tok, verbose, prompt, n_new_tokens, layer, use_sampling)\u001b[0m\n\u001b[1;32m    169\u001b[0m         l2toks[layer_idx] \u001b[38;5;241m=\u001b[39m decoded_texts\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# Now probas_list has length == number of tested layers\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# Each element is shape (batch_size, n_new_tokens, vocab_size)\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# Stack them => (n_layers_tested, batch_size, n_new_tokens, vocab_size)\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m probas \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobas_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m l2toks, probas\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1, 13, 256000] at entry 0 and [1, 18, 256000] at entry 1"
     ]
    }
   ],
   "source": [
    "\n",
    "steering_vec = t.load('llm-localization/gemma2_9b_it/universal/en_universal_ru_out.pt')\n",
    "\n",
    "out = contrastive_act_gen_opt(nnmodel, tokenizer, alpha * steering_vec.unsqueeze(1), prompt=prompt, layer=[21,22,23,24,25], n_new_tokens=50, use_sampling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mech-int",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
